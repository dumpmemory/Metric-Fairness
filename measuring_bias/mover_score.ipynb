{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2022-03-29T04:54:29.960748Z","iopub.status.busy":"2022-03-29T04:54:29.960115Z","iopub.status.idle":"2022-03-29T04:54:41.901197Z","shell.execute_reply":"2022-03-29T04:54:41.899998Z","shell.execute_reply.started":"2022-03-29T04:54:29.960651Z"},"trusted":true},"outputs":[],"source":["from __future__ import absolute_import, division, print_function\n","import numpy as np\n","import torch\n","import string\n","import os\n","from pyemd import emd, emd_with_flow\n","from torch import nn\n","from math import log\n","from itertools import chain\n","\n","from collections import defaultdict, Counter\n","from multiprocessing import Pool\n","from functools import partial\n","\n","\n","from transformers import AutoTokenizer, AutoModel\n","\n","if os.environ.get('MOVERSCORE_MODEL'):\n","    model_name = os.environ.get('MOVERSCORE_MODEL')\n","else:\n","    model_name = 'distilbert-base-uncased'\n","tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=True)\n","model = AutoModel.from_pretrained(model_name, output_hidden_states=True, output_attentions=True)\n","model.eval()\n","\n","\n","def truncate(tokens):\n","    if len(tokens) > tokenizer.model_max_length - 2:\n","        tokens = tokens[0:(tokenizer.model_max_length - 2)]\n","    return tokens\n","\n","def process(a):\n","    a = [\"[CLS]\"]+truncate(tokenizer.tokenize(a))+[\"[SEP]\"]\n","    a = tokenizer.convert_tokens_to_ids(a)\n","    return set(a)\n","\n","\n","def get_idf_dict(arr, nthreads=4):\n","    idf_count = Counter()\n","    num_docs = len(arr)\n","\n","    process_partial = partial(process)\n","\n","    with Pool(nthreads) as p:\n","        idf_count.update(chain.from_iterable(p.map(process_partial, arr)))\n","\n","    idf_dict = defaultdict(lambda : log((num_docs+1)/(1)))\n","    idf_dict.update({idx:log((num_docs+1)/(c+1)) for (idx, c) in idf_count.items()})\n","    return idf_dict\n","\n","def padding(arr, pad_token, dtype=torch.long):\n","    lens = torch.LongTensor([len(a) for a in arr])\n","    max_len = lens.max().item()\n","    padded = torch.ones(len(arr), max_len, dtype=dtype) * pad_token\n","    mask = torch.zeros(len(arr), max_len, dtype=torch.long)\n","    for i, a in enumerate(arr):\n","        padded[i, :lens[i]] = torch.tensor(a, dtype=dtype)\n","        mask[i, :lens[i]] = 1\n","    return padded, lens, mask\n","\n","def bert_encode(model, x, attention_mask):\n","    model.eval()\n","    with torch.no_grad():\n","        result = model(x, attention_mask = attention_mask)\n","    if model_name == 'distilbert-base-uncased':\n","        return result[1] \n","    else:\n","        return result[2] \n","\n","#with open('stopwords.txt', 'r', encoding='utf-8') as f:\n","#    stop_words = set(f.read().strip().split(' '))\n","\n","def collate_idf(arr, tokenize, numericalize, idf_dict,\n","                pad=\"[PAD]\"):\n","    \n","    tokens = [[\"[CLS]\"]+truncate(tokenize(a))+[\"[SEP]\"] for a in arr]  \n","    arr = [numericalize(a) for a in tokens]\n","\n","    idf_weights = [[idf_dict[i] for i in a] for a in arr]\n","    \n","    pad_token = numericalize([pad])[0]\n","\n","    padded, lens, mask = padding(arr, pad_token, dtype=torch.long)\n","    padded_idf, _, _ = padding(idf_weights, pad_token, dtype=torch.float)\n","\n","    return padded, padded_idf, lens, mask, tokens\n","\n","def get_bert_embedding(all_sens, model, tokenizer, idf_dict,\n","                       batch_size=-1):\n","\n","    padded_sens, padded_idf, lens, mask, tokens = collate_idf(all_sens,\n","                                                      tokenizer.tokenize, tokenizer.convert_tokens_to_ids,\n","                                                      idf_dict)\n","\n","    if batch_size == -1: batch_size = len(all_sens)\n","\n","    embeddings = []\n","    with torch.no_grad():\n","        for i in range(0, len(all_sens), batch_size):\n","            batch_embedding = bert_encode(model, padded_sens[i:i+batch_size],\n","                                          attention_mask=mask[i:i+batch_size])\n","            batch_embedding = torch.stack(batch_embedding)\n","            embeddings.append(batch_embedding)\n","            del batch_embedding\n","\n","    total_embedding = torch.cat(embeddings, dim=-3)\n","    return total_embedding, lens, mask, padded_idf, tokens\n","\n","def _safe_divide(numerator, denominator):\n","    return numerator / (denominator + 1e-30)\n","\n","def batched_cdist_l2(x1, x2):\n","    x1_norm = x1.pow(2).sum(dim=-1, keepdim=True)\n","    x2_norm = x2.pow(2).sum(dim=-1, keepdim=True)\n","    res = torch.baddbmm(\n","        x2_norm.transpose(-2, -1),\n","        x1,\n","        x2.transpose(-2, -1),\n","        alpha=-2\n","    ).add_(x1_norm).clamp_min_(1e-30).sqrt_()\n","    return res\n","\n","def word_mover_score(refs, hyps, idf_dict_ref, idf_dict_hyp, stop_words=[], n_gram=1, remove_subwords = True, batch_size=256):\n","    preds = []\n","    for batch_start in range(0, len(refs), batch_size):\n","        batch_refs = refs[batch_start:batch_start+batch_size]\n","        batch_hyps = hyps[batch_start:batch_start+batch_size]\n","        \n","        ref_embedding, ref_lens, ref_masks, ref_idf, ref_tokens = get_bert_embedding(batch_refs, model, tokenizer, idf_dict_ref)\n","        hyp_embedding, hyp_lens, hyp_masks, hyp_idf, hyp_tokens = get_bert_embedding(batch_hyps, model, tokenizer, idf_dict_hyp)\n","\n","        ref_embedding = ref_embedding[-1]\n","        hyp_embedding = hyp_embedding[-1]\n","        \n","        batch_size = len(ref_tokens)\n","        for i in range(batch_size):  \n","            ref_ids = [k for k, w in enumerate(ref_tokens[i]) \n","                                if w in stop_words or '##' in w \n","                                or w in set(string.punctuation)]\n","            hyp_ids = [k for k, w in enumerate(hyp_tokens[i]) \n","                                if w in stop_words or '##' in w\n","                                or w in set(string.punctuation)]\n","          \n","            ref_embedding[i, ref_ids,:] = 0                        \n","            hyp_embedding[i, hyp_ids,:] = 0\n","            \n","            ref_idf[i, ref_ids] = 0\n","            hyp_idf[i, hyp_ids] = 0\n","            \n","        raw = torch.cat([ref_embedding, hyp_embedding], 1)\n","                             \n","        raw.div_(torch.norm(raw, dim=-1).unsqueeze(-1) + 1e-30) \n","        \n","        distance_matrix = batched_cdist_l2(raw, raw).double().cpu().numpy()\n","                \n","        for i in range(batch_size):  \n","            c1 = np.zeros(raw.shape[1], dtype=np.float)\n","            c2 = np.zeros(raw.shape[1], dtype=np.float)\n","            c1[:len(ref_idf[i])] = ref_idf[i]\n","            c2[len(ref_idf[i]):] = hyp_idf[i]\n","            \n","            c1 = _safe_divide(c1, np.sum(c1))\n","            c2 = _safe_divide(c2, np.sum(c2))\n","            \n","            dst = distance_matrix[i]\n","            _, flow = emd_with_flow(c1, c2, dst)\n","            flow = np.array(flow, dtype=np.float32)\n","            score = 1./(1. + np.sum(flow * dst))#1 - np.sum(flow * dst)\n","            preds.append(score)\n","\n","    return preds\n","\n","import matplotlib.pyplot as plt\n","\n","def plot_example(is_flow, reference, translation, device='cuda:0'):\n","    \n","    idf_dict_ref = defaultdict(lambda: 1.) \n","    idf_dict_hyp = defaultdict(lambda: 1.)\n","    \n","    ref_embedding, ref_lens, ref_masks, ref_idf, ref_tokens = get_bert_embedding([reference], model, tokenizer, idf_dict_ref)\n","    hyp_embedding, hyp_lens, hyp_masks, hyp_idf, hyp_tokens = get_bert_embedding([translation], model, tokenizer, idf_dict_hyp)\n","   \n","    ref_embedding = ref_embedding[-1]\n","    hyp_embedding = hyp_embedding[-1]\n","               \n","    raw = torch.cat([ref_embedding, hyp_embedding], 1)            \n","    raw.div_(torch.norm(raw, dim=-1).unsqueeze(-1) + 1e-30) \n","    \n","    distance_matrix = batched_cdist_l2(raw, raw)\n","    masks = torch.cat([ref_masks, hyp_masks], 1)        \n","    masks = torch.einsum('bi,bj->bij', (masks, masks))\n","    distance_matrix = masks * distance_matrix              \n","\n","    \n","    i = 0\n","    c1 = np.zeros(raw.shape[1], dtype=np.float)\n","    c2 = np.zeros(raw.shape[1], dtype=np.float)\n","    c1[:len(ref_idf[i])] = ref_idf[i]\n","    c2[len(ref_idf[i]):] = hyp_idf[i]\n","    \n","    c1 = _safe_divide(c1, np.sum(c1))\n","    c2 = _safe_divide(c2, np.sum(c2))\n","    \n","    dst = distance_matrix[i].double().cpu().numpy()\n","\n","    if is_flow:        \n","        _, flow = emd_with_flow(c1, c2, dst)\n","        new_flow = np.array(flow, dtype=np.float32)    \n","        res = new_flow[:len(ref_tokens[i]), len(ref_idf[i]): (len(ref_idf[i])+len(hyp_tokens[i]))]\n","    else:    \n","        res = 1./(1. + dst[:len(ref_tokens[i]), len(ref_idf[i]): (len(ref_idf[i])+len(hyp_tokens[i]))]) \n","\n","    r_tokens = ref_tokens[i]\n","    h_tokens = hyp_tokens[i]\n","    \n","    fig, ax = plt.subplots(figsize=(len(r_tokens)*0.8, len(h_tokens)*0.8))\n","    im = ax.imshow(res, cmap='Blues')\n","    \n","    ax.set_xticks(np.arange(len(h_tokens)))\n","    ax.set_yticks(np.arange(len(r_tokens)))\n","  \n","    ax.set_xticklabels(h_tokens, fontsize=10)\n","    ax.set_yticklabels(r_tokens, fontsize=10)\n","    plt.xlabel(\"System Translation\", fontsize=14)\n","    plt.ylabel(\"Human Reference\", fontsize=14)\n","    plt.title(\"Flow Matrix\", fontsize=14)\n","    \n","    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n","             rotation_mode=\"anchor\")\n","\n","#    for i in range(len(r_tokens)):\n","#        for j in range(len(h_tokens)):\n","#            text = ax.text(j, i, '{:.2f}'.format(res[i, j].item()),\n","#                           ha=\"center\", va=\"center\", color=\"k\" if res[i, j].item() < 0.6 else \"w\")    \n","    fig.tight_layout()\n","    plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2022-03-29T04:57:37.374886Z","iopub.status.busy":"2022-03-29T04:57:37.374376Z"},"trusted":true},"outputs":[],"source":["\n","dataset_list=['race-bias','bertscore','religionbias','physical-bias','agebias','disabilitybias','socioeconomicbias']\n","for path in dataset_list:\n","    with open('../input/'+path + \"/hyps.txt\") as f:\n","        translations = [line.strip() for line in f]\n","\n","    with open('../input/'+path + \"/refs.txt\") as f:\n","        references = [line.strip() for line in f]\n","    idf_dict_hyp = get_idf_dict(translations) # idf_dict_hyp = defaultdict(lambda: 1.)\n","    idf_dict_ref = get_idf_dict(references) # idf_dict_ref = defaultdict(lambda: 1.)\n","\n","    scores = word_mover_score(references, translations, idf_dict_ref, idf_dict_hyp, \\\n","                          stop_words=[], n_gram=2, remove_subwords=True)\n","    with open('./' + path+'.txt','w') as f:\n","        for i in scores:\n","            f.write(str(i))\n","            f.write('\\n')"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"}},"nbformat":4,"nbformat_minor":4}
